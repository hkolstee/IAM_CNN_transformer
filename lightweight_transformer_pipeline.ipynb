{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "SAG_Ci6DBXKc"
      },
      "source": [
        "# Lightweight Transformer-based Model for Handwritten Character Recognition\n",
        "(https://hal.science/hal-03685976/file/A_Light_Transformer_Based_Architecture_for_Handwritten_Text_Recognition.pdf)\n",
        "\n",
        "## ***note: Has a CNN backbone***\n",
        "\n",
        "-----------"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "JXgJqgIzBXKe"
      },
      "source": [
        "## Architecture\n",
        "Build up with a double Transformer architecture:\n",
        "- Image transformer as encoder: Extracts the visual features\n",
        "- Text transformer as decoder: Language modeling, generates word-sections sequence using visual features and previous predictions\n",
        "\n",
        "### Encoder:\n",
        "- CNN Backbone (5 convolutions)\n",
        "- Sinusodial position encoding\n",
        "- 4 layer transformer layer encoder\n",
        "\n",
        "### Decoder:\n",
        "- Takes encoder output and along with sequence of previously predicted characters\n",
        "- Additional loss in the middle of the network to help convergence\n",
        "\n",
        "### Hybrid loss:\n",
        "- CTC and CE Loss combined, CTC on intermediate encoder output, CE on decoder output\n",
        "\n",
        "--------------\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "GDGJG_0NBXKf"
      },
      "source": [
        "## Start pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "GOEacPE6BXKf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-06-13 23:29:34.280514: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "from collections import OrderedDict, Counter\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "import torchvision as tv\n",
        "from torchvision.io import read_image\n",
        "\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# PyTorch TensorBoard support\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from datetime import datetime\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "import imgaug.augmenters as iaa\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "cerLK3MkM-YY"
      },
      "outputs": [],
      "source": [
        "torch.multiprocessing.set_start_method('spawn', force = True)# good solution !!!!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "-hENzWhBBXKg"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# pytorch device we want to run the tensor calculations on\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "# device = \"cpu\"\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "bBWZpTD2Bft4"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'google.colab'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgoogle\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcolab\u001b[39;00m \u001b[39mimport\u001b[39;00m drive\n\u001b[1;32m      2\u001b[0m drive\u001b[39m.\u001b[39mmount(\u001b[39m'\u001b[39m\u001b[39m/content/drive\u001b[39m\u001b[39m'\u001b[39m)\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "cV56kos0BXKh"
      },
      "source": [
        "### Constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bYzrm9b_BXKh"
      },
      "outputs": [],
      "source": [
        "DATA_PATH = f\"/content/IAM-data/\"\n",
        "# DATA_PATH = f\"/home/hkolstee/uniprojects/DATA/HWR/IAM-data/IAM-data/\"\n",
        "TRAIN_TEST_SPLIT = 0.05\n",
        "# BATCH_SIZE = 4\n",
        "BATCH_SIZE = 32\n",
        "INPUT_HEIGHT = 128\n",
        "# input width -> largest width in batch\n",
        "#   images max resized and subsequently padded to get to width\n",
        "EPOCHS = 1500"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "PQm3UHBJBXKh"
      },
      "source": [
        "### Prepare Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "snBliCaElJph"
      },
      "outputs": [],
      "source": [
        "!unzip -o \"/content/drive/MyDrive/IAM-data.zip\" -d \"/content\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YWeAEZaABXKh"
      },
      "outputs": [],
      "source": [
        "raw_data = pd.read_fwf(DATA_PATH + \"iam_lines_gt.txt\", header = None)\n",
        "raw_data = raw_data.values.tolist()\n",
        "print(len(raw_data))\n",
        "\n",
        "data = {'img_names': np.squeeze(raw_data[::2]),\n",
        "        'labels': np.squeeze(raw_data[1::2])}\n",
        "\n",
        "data = pd.DataFrame(data)\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sknd_f6NBXKi"
      },
      "outputs": [],
      "source": [
        "# example (label indx unknown)\n",
        "ex_label = \"an Italian who is perhaps the best Valet de Chambre\"\n",
        "print(ex_label)\n",
        "image = read_image(os.path.join(DATA_PATH, \"img\", \"g06-026i-01.png\"))\n",
        "plt.imshow(image[0, :, :], cmap = \"gray\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "1zjze21yBXKj"
      },
      "source": [
        "first, we need the input/image width we have to resize the images to.\n",
        "This is the largest image width in the entire batch of images (source paper randomly added/removed new augments each training epoch).\n",
        "For now we just take the largest width in the original images.\n",
        "\n",
        "The labels are later padded to the largest label size in the dataset, such that we also need to know the longest label size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nb-BV_EnBXKj"
      },
      "outputs": [],
      "source": [
        "def getBiggestWidth(data: pd.DataFrame):\n",
        "    biggest_width = 0\n",
        "\n",
        "    for index in range(len(data['img_names'])):\n",
        "        image_path = os.path.join(DATA_PATH, \"img\", data['img_names'][index])\n",
        "        image = read_image(image_path)\n",
        "\n",
        "        if (image.size(2) > biggest_width):\n",
        "            biggest_width = image.size(2)\n",
        "\n",
        "    return biggest_width\n",
        "\n",
        "def getLongestLabel(data: pd.DataFrame):\n",
        "    longest = 0\n",
        "\n",
        "    for index in range(len(data['labels'])):\n",
        "        if (len(data['labels'][index]) > longest):\n",
        "            longest = len(data['labels'][index])\n",
        "\n",
        "    return longest"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "C2CkaYcyBXKj"
      },
      "source": [
        "Biggest width needed to pad all images to this width for the input into the encoder.\n",
        "Longest label needed to pad all labels to this length for the input into the decoder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I5TFh_UCBXKj"
      },
      "outputs": [],
      "source": [
        "input_width = getBiggestWidth(data)\n",
        "print(input_width)\n",
        "\n",
        "longest_label = getLongestLabel(data)\n",
        "# <BOS> and <EOS> tokens not counted\n",
        "longest_label += 2\n",
        "print(longest_label)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "t82UVnH3BXKk"
      },
      "source": [
        "### Resize and pad images to largest width in dataset\n",
        "### ***NOTE: Not sure if padding should be all at the right part of the image or both sides***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U1YzCBZqBXKk"
      },
      "outputs": [],
      "source": [
        "# resizes to largest width in batch x 128, keeping aspect ratio and padding image\n",
        "class resizeImage(object):\n",
        "    def __init__(self, resize_width, resize_height):\n",
        "        self.resize_width = resize_width\n",
        "        self.resize_height = resize_height\n",
        "\n",
        "    def __call__(self, image):\n",
        "        # check if resizing to correct height while keeping aspect ratio does not overshoot correct width\n",
        "        aspect_ratio_width = int((self.resize_height / image.shape[1]) * image.shape[2])\n",
        "        if (aspect_ratio_width > self.resize_width):\n",
        "            # calculate max ratio of change for not overshooting resize width while keeping aspect ratio\n",
        "            max_ratio = self.resize_width / image.shape[2]\n",
        "            max_resize_height = int(max_ratio * image.shape[1])\n",
        "            # calc up and down padding\n",
        "            padding_up = int(((self.resize_height - max_resize_height) / 2))\n",
        "            padding_down = self.resize_height - max_resize_height - padding_up\n",
        "            # change resize height to max calculated resize height\n",
        "            new_resize_height = max_resize_height\n",
        "        else:\n",
        "            padding_up = 0\n",
        "            padding_down = 0\n",
        "            new_resize_height = self.resize_height\n",
        "\n",
        "        # resize to correct image height, while keeping aspect ratio\n",
        "        resize_transform = tv.transforms.Resize((new_resize_height, self.resize_width), antialias = True)\n",
        "\n",
        "        if isinstance(image, np.ndarray):\n",
        "          image = resize_transform(torch.tensor(image, dtype = torch.float32))\n",
        "        else:\n",
        "          image = resize_transform(image)\n",
        "\n",
        "        # pad to correct width (and height if necessary)\n",
        "        padding_left = int(((self.resize_width - image.shape[2]) / 2))\n",
        "        padding_right = self.resize_width - image.shape[2] - padding_left\n",
        "        image = F.pad(image, (padding_left, padding_right, padding_up, padding_down), mode = \"constant\", value = 255)\n",
        "\n",
        "        return image"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "CBHvzRxGBXKk"
      },
      "source": [
        "\n",
        "This example because it previously overshot the correct width using the height measurements for the aspect ratio."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9qhSOhD8BXKl"
      },
      "outputs": [],
      "source": [
        "print(ex_label)\n",
        "image = read_image(os.path.join(DATA_PATH, \"img\", \"g06-026i-01.png\"))\n",
        "print(image.shape)\n",
        "\n",
        "resize_transform = resizeImage(input_width, INPUT_HEIGHT)\n",
        "resized_image = resize_transform(image)\n",
        "plt.imshow(resized_image[0, :, :], cmap = \"gray\")\n",
        "print(resized_image.shape)\n",
        "print(resized_image.dtype)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "k7TQKJdTBXKl"
      },
      "source": [
        "Resize and pad for the whole dataset and save in a new dir such that we don't have to do this each epoch. TODO"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "gd23zavvBXKl"
      },
      "source": [
        "### Train test split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1nVsFzD_BXKl"
      },
      "outputs": [],
      "source": [
        "train_data, test_data = train_test_split(data, test_size = TRAIN_TEST_SPLIT)\n",
        "\n",
        "# reset indices from current random state\n",
        "train_data.reset_index(inplace = True)\n",
        "test_data.reset_index(inplace = True)\n",
        "\n",
        "print(train_data)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "WutIuAwFBXKm"
      },
      "source": [
        "### Create custom pytorch dataset"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ZNM_StGTBXKm"
      },
      "source": [
        "\n",
        "For character level embedding (decoder input) we find out how many characters are present in the dataset. By counting the characters we give the most common characters the lowest indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BMXwq3dJBXKm"
      },
      "outputs": [],
      "source": [
        "# returns a dict of uniques chars sorted on how common they are in the dataset labels\n",
        "def uniqueCharsByMostCommon(data: pd.DataFrame):\n",
        "    sortedDict = OrderedDict(Counter(''.join(data['labels'].values)).most_common())\n",
        "    newDict = {}\n",
        "\n",
        "    # first add pad, begin of sentence, and end of sentence tokens\n",
        "    newDict[\"<PAD>\"] = 0\n",
        "    newDict[\"<BOS>\"] = 1\n",
        "    newDict[\"<EOS>\"] = 2\n",
        "\n",
        "    for idx, char in enumerate(sortedDict):\n",
        "        newDict[char] = idx + 3\n",
        "\n",
        "    return newDict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vDtznjjKBXKm"
      },
      "outputs": [],
      "source": [
        "# get mapping before splitting dataset\n",
        "char_to_idx_mapping = uniqueCharsByMostCommon(data)\n",
        "char_to_idx_mapping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5gTYzC8MLJ8Q"
      },
      "outputs": [],
      "source": [
        "# Function to calc the mean and std of a dataset to use in image standardization\n",
        "def calcImagesMeanStd(data):\n",
        "    running_mean = 0\n",
        "    running_std = 0\n",
        "\n",
        "    for image_name in data['img_names']:\n",
        "        image = read_image(os.path.join(DATA_PATH, \"img\", image_name)).float()\n",
        "\n",
        "        mean = torch.mean(image)\n",
        "        std = torch.std(image)\n",
        "\n",
        "        running_mean += mean\n",
        "        running_std += std\n",
        "\n",
        "    return (running_mean / len(data)), (running_std / len(data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pro43TyNLJ8R"
      },
      "outputs": [],
      "source": [
        "mean, std = calcImagesMeanStd(data)\n",
        "mean, std"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x6Y6OTFjLJ8R"
      },
      "outputs": [],
      "source": [
        "# paper adds gaussian noise to image (sqrt(0.1) * rand from gaussian distri)\n",
        "class addGaussianNoise(object):\n",
        "    def __call__(self, image):\n",
        "        image += ((0.1**0.5) * torch.randn(image.shape)) * 0.75\n",
        "\n",
        "        return image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fLrrmWz4v6i2"
      },
      "outputs": [],
      "source": [
        "class elasticTransform(object):\n",
        "    def __init__(self):\n",
        "        self.elastic = iaa.ElasticTransformation(alpha = 800, sigma = 33, mode = \"nearest\")\n",
        "\n",
        "    def __call__(self, image):\n",
        "        image = self.elastic.augment_image(image.squeeze())\n",
        "        return np.expand_dims(image, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2hgU-s-qv6i2"
      },
      "outputs": [],
      "source": [
        "class randomPad(object):\n",
        "    def __call__(self, image):\n",
        "        i = np.random.randint(0, 1)\n",
        "        if i == 0:\n",
        "            image = np.pad(image.squeeze(), ((0, 0), (np.random.randint(0, 200), 0)), constant_values = 255)\n",
        "        else:\n",
        "            image = np.pad(image.squeeze(), ((0, 0), (0, np.random.randint(0, 200))), constant_values = 255)\n",
        "        return np.expand_dims(image, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s6r2VhWWBXKn"
      },
      "outputs": [],
      "source": [
        "class HandWritingDataset(Dataset):\n",
        "    def __init__(self, eval : bool, data: pd.DataFrame, data_path, img_width, img_height, char_to_idx_mapping: dict, max_label_size, mean, std):\n",
        "        self.data = data\n",
        "        self.data_path = data_path\n",
        "        self.img_width = img_width\n",
        "        self.img_height = img_height\n",
        "        self.label_size = max_label_size\n",
        "\n",
        "        self.char_to_idx_mapping = char_to_idx_mapping\n",
        "        self.idx_to_char_mapping = {value: key for key, value in self.char_to_idx_mapping.items()}\n",
        "\n",
        "        # composed transform\n",
        "        self.transforms_train = tv.transforms.Compose([randomPad(),\n",
        "                                                       elasticTransform(),\n",
        "                                                       resizeImage(img_width, img_height),\n",
        "                                                       tv.transforms.Normalize(mean, std),\n",
        "                                                       addGaussianNoise()])\n",
        "\n",
        "        self.transforms_eval = tv.transforms.Compose([resizeImage(img_width, img_height),\n",
        "                                                      tv.transforms.Normalize(mean, std),\n",
        "                                                      addGaussianNoise()])\n",
        "\n",
        "        self.createLabelEncodings()\n",
        "        self.transformAndStoreImages(eval)\n",
        "\n",
        "    def createLabelEncodings(self):\n",
        "        # change character level strings to embed indices\n",
        "        #   (embedding itself calculated in forward pass)\n",
        "        self.labels_as_idxs = [torch.tensor([[self.char_to_idx_mapping[char]] for char in label]) for label in self.data['labels']]\n",
        "        # add <EOS> tokens at the end of sentences\n",
        "        self.labels_as_idxs = [torch.cat([label, torch.tensor([[self.char_to_idx_mapping['<EOS>']]])]) for label in self.labels_as_idxs]\n",
        "        # input into decoder shifted right (while training) and <BOS> token inserted at start\n",
        "        self.decoder_input_as_idxs = [torch.cat([torch.tensor([[self.char_to_idx_mapping['<BOS>']]]), label]) for label in self.labels_as_idxs]\n",
        "\n",
        "        # target lengths for CTC loss\n",
        "        # self.labels_lengths = [torch.tensor(len(label)).to(device) for label in self.labels_as_idxs]\n",
        "        self.labels_lengths = [torch.tensor(len(label)) for label in self.labels_as_idxs]\n",
        "\n",
        "        # pad labels embedding indices to largest label length with <PAD> token\n",
        "        self.labels_as_idxs = [F.pad(label, (0, 0, 0, self.label_size - label.shape[0]), mode = 'constant', value = self.char_to_idx_mapping['<PAD>']) \\\n",
        "        # self.labels_as_idxs = [F.pad(label, (0, 0, 0, self.label_size - label.shape[0]), mode = 'constant', value = self.char_to_idx_mapping['<PAD>']).to(device) \\\n",
        "                               for label in self.labels_as_idxs]\n",
        "\n",
        "        # pad decoder input char embedding indices to largest label length with <PAD> tokens\n",
        "        self.decoder_input_as_idxs = [F.pad(label, (0, 0, 0, self.label_size - label.shape[0]), mode = 'constant', value = self.char_to_idx_mapping['<PAD>']) \\\n",
        "        # self.decoder_input_as_idxs = [F.pad(label, (0, 0, 0, self.label_size - label.shape[0]), mode = 'constant', value = self.char_to_idx_mapping['<PAD>']).to(device) \\\n",
        "                               for label in self.decoder_input_as_idxs]\n",
        "\n",
        "        # transform target labels into one hot encoding vectors\n",
        "        self.labels_as_onehot =  [F.one_hot(label, num_classes = len(self.char_to_idx_mapping)).squeeze().float() for label in self.labels_as_idxs]\n",
        "        # self.labels_as_onehot =  [F.one_hot(label, num_classes = len(self.char_to_idx_mapping)).squeeze().float().to(device) for label in self.labels_as_idxs]\n",
        "\n",
        "    # reshapes + pads the images to the correct input width and height\n",
        "    def transformAndStoreImages(self, eval):\n",
        "        self.images = []\n",
        "\n",
        "        for image_name in self.data['img_names']:\n",
        "            # read image\n",
        "            image = read_image(os.path.join(self.data_path, \"img\", image_name)).float()\n",
        "            # resize + pad image to correct input size\n",
        "            if eval:\n",
        "              image = self.transforms_eval(image)\n",
        "            else:\n",
        "              image = self.transforms_train(image)\n",
        "            # store in list of tensors\n",
        "            self.images.append(image.to(device))\n",
        "\n",
        "    # transforms done in __getitem__ so that images can be stored as Byte tensors (-> less memory and kernel does not crash)\n",
        "    def __getitem__(self, index):\n",
        "        image = self.images[index]\n",
        "\n",
        "        # label\n",
        "        encoder_label = self.labels_as_idxs[index].squeeze()\n",
        "        decoder_label = self.labels_as_onehot[index]\n",
        "            # used for CTC Loss\n",
        "        label_length = self.labels_lengths[index]\n",
        "        # label shifted right\n",
        "        decoder_in = self.decoder_input_as_idxs[index].squeeze()\n",
        "\n",
        "        # image = tensor, label = one hot encoded target characters, decoder_in = label shifted right as indices for embedding table\n",
        "        return image, encoder_label, label_length, decoder_label, decoder_in\n",
        "\n",
        "    def __len__(self):\n",
        "        # return length of column\n",
        "        return len(self.data)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Pfriu4cABXKn"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "GGkTu8kgBXKn"
      },
      "source": [
        "Sinusodial positional encoding\n",
        "(can be changed to nn.embedding layers if we don't get good results, however that is not exactly sinusodial pos encoding like in the paper I think)\n",
        "\n",
        "<!-- **CHANGED TO NN.EMBEDDING IN MODEL**   -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9FcI2USeBXKo"
      },
      "outputs": [],
      "source": [
        "class SinPosEncoding(nn.Module):\n",
        "    def __init__(self, dimensionality):\n",
        "        super(SinPosEncoding, self).__init__()\n",
        "        self.dimensionality = dimensionality\n",
        "        self.max_len = 1000\n",
        "\n",
        "        # initialize encodings\n",
        "        pos_encodings = torch.zeros((self.max_len, 1, self.dimensionality), dtype = torch.float32)\n",
        "\n",
        "        # loop through size of matrix\n",
        "        for j in range(self.max_len):\n",
        "            for i in np.arange(int(self.dimensionality / 2)):\n",
        "                # calc sin and cos wave offsets (alternated) for each row of matrix\n",
        "                pos_encodings[j, 0, 2*i] = np.sin(j / (self.max_len ** ((2*i) / self.dimensionality)))\n",
        "                pos_encodings[j, 0, 2*i + 1] = np.cos(j / (self.max_len ** ((2*i) / self.dimensionality)))\n",
        "\n",
        "        # add to buffer for performance (?)\n",
        "        self.register_buffer('pos_encodings', pos_encodings)\n",
        "\n",
        "    def forward(self, input: torch.Tensor):\n",
        "        # adds the positional encoding elementwise to the tensor (seqlength, batch, embeddims)\n",
        "        input += self.pos_encodings[0:input.size(0)]\n",
        "\n",
        "        return input"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Qu-71iaSBXKo"
      },
      "source": [
        "Model architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TrEpuQeEBXKo"
      },
      "outputs": [],
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self, input_height, input_width):\n",
        "        super(CNN, self).__init__()\n",
        "        # convolutional block (5 convolutions)\n",
        "        # first convolution\n",
        "        self.conv1 = nn.Conv2d(in_channels = 1, out_channels = 8, kernel_size = (3,3))\n",
        "        width = input_width - 2\n",
        "        height = input_height - 2\n",
        "        self.leakyRelu = nn.LeakyReLU()     # reuse in later layers\n",
        "        self.maxPool = nn.MaxPool2d((2,2))  # reuse in later layers\n",
        "        width = int(np.floor(width/2))\n",
        "        height = int(np.floor(height/2))\n",
        "        self.layerNorm1 = nn.LayerNorm(normalized_shape = [8, height, width])\n",
        "        self.dropout = nn.Dropout(0.2)      # reuse in later layers\n",
        "\n",
        "        # second convolutional layer\n",
        "        self.conv2 = nn.Conv2d(in_channels = 8, out_channels = 16, kernel_size = (3, 3))\n",
        "        width -= 2\n",
        "        height -= 2\n",
        "        # after maxpool\n",
        "        width = int(np.floor(width/2))\n",
        "        height = int(np.floor(height/2))\n",
        "        self.layerNorm2 = nn.LayerNorm(normalized_shape = [16, height, width])\n",
        "\n",
        "        # third convolutional layer\n",
        "        self.conv3 = nn.Conv2d(in_channels = 16, out_channels = 32, kernel_size = (3, 3))\n",
        "        width -= 2\n",
        "        height -= 2\n",
        "        # after maxpool\n",
        "        width = int(np.floor(width/2))\n",
        "        height = int(np.floor(height/2))\n",
        "        self.layerNorm3 = nn.LayerNorm(normalized_shape = [32, height, width])\n",
        "\n",
        "        # forth convolutional layer\n",
        "        self.conv4 = nn.Conv2d(in_channels = 32, out_channels = 64, kernel_size = (3, 3))\n",
        "        width -= 2\n",
        "        height -= 2\n",
        "        # no maxpool\n",
        "        self.layerNorm4 = nn.LayerNorm(normalized_shape = [64, height, width])\n",
        "\n",
        "        # fifth convolutional layer (kernel size to better match shape of character)\n",
        "        self.conv5 = nn.Conv2d(in_channels = 64, out_channels = 128, kernel_size = (4, 2))\n",
        "        width -= 1\n",
        "        height -= 3\n",
        "        # no maxpool\n",
        "        self.layerNorm5 = nn.LayerNorm(normalized_shape = [128, height, width])\n",
        "\n",
        "        # following is convolution with width 1 which is used to flatten the current output\n",
        "        self.flattenConv = nn.Conv2d(in_channels = 128, out_channels = 128, kernel_size = (height, 1))\n",
        "        self.layerNorm6 = nn.LayerNorm(normalized_shape = [128, 1, width])\n",
        "\n",
        "        # dense layer to upscale from 128 to 256\n",
        "        self.dense = nn.Linear(in_features = 128, out_features = 256)\n",
        "\n",
        "    def forward(self, input_img):\n",
        "        # first conv\n",
        "        conv_out = self.layerNorm1(self.maxPool(self.leakyRelu(self.conv1(input_img))))\n",
        "        conv_out = self.dropout(conv_out)\n",
        "        # second conv\n",
        "        conv_out = self.layerNorm2(self.maxPool(self.leakyRelu(self.conv2(conv_out))))\n",
        "        conv_out = self.dropout(conv_out)\n",
        "        # third conv\n",
        "        conv_out = self.layerNorm3(self.maxPool(self.leakyRelu(self.conv3(conv_out))))\n",
        "        conv_out = self.dropout(conv_out)\n",
        "        # forth conv\n",
        "        conv_out = self.layerNorm4(self.leakyRelu(self.conv4(conv_out)))\n",
        "        # fifth conv\n",
        "        conv_out = self.layerNorm5(self.leakyRelu(self.conv5(conv_out)))\n",
        "\n",
        "        # flatten layer\n",
        "        conv_out = self.layerNorm6(self.leakyRelu(self.flattenConv(conv_out)))\n",
        "\n",
        "        # reshape from ((batch, 128, 1, x) -> (batch, x, 1, 128)) for dense layer\n",
        "        conv_out = torch.permute(conv_out, (0, 3, 2, 1))\n",
        "\n",
        "        # upscale from 128 to 256\n",
        "        conv_out = self.dense(conv_out)\n",
        "\n",
        "        # reshape to (seq, batch, embed_dim)\n",
        "        conv_out = torch.permute(conv_out.squeeze(2), (1, 0, 2))\n",
        "\n",
        "        return conv_out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ludlAHsBXKo"
      },
      "outputs": [],
      "source": [
        "class HWRTransformerEncoder(nn.Module):\n",
        "    def __init__(self, total_nr_of_tokens):\n",
        "        super(HWRTransformerEncoder, self).__init__()\n",
        "        # transformer encoder layers (4 stacked transformer encoder layers (4 headed attention))\n",
        "        self.trans_encoder1 = nn.TransformerEncoderLayer(d_model = 256, nhead = 4, dim_feedforward = 1024, dropout = 0.2)\n",
        "        self.trans_encoder2 = nn.TransformerEncoderLayer(d_model = 256, nhead = 4, dim_feedforward = 1024, dropout = 0.2)\n",
        "        self.trans_encoder3 = nn.TransformerEncoderLayer(d_model = 256, nhead = 4, dim_feedforward = 1024, dropout = 0.2)\n",
        "        self.trans_encoder4 = nn.TransformerEncoderLayer(d_model = 256, nhead = 4, dim_feedforward = 1024, dropout = 0.2)\n",
        "\n",
        "        # dense layer for backprop CTC Loss of intermediate encoder result\n",
        "        self.encoder_out_dense = nn.Linear(256, total_nr_of_tokens)\n",
        "\n",
        "    def forward(self, encoder_input):\n",
        "        # transformer encoder layers\n",
        "        encoder_out = self.trans_encoder1(encoder_input)\n",
        "        encoder_out = self.trans_encoder2(encoder_out)\n",
        "        encoder_out = self.trans_encoder3(encoder_out)\n",
        "        encoder_out = self.trans_encoder4(encoder_out)\n",
        "\n",
        "        return encoder_out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qhtSiH0kBXKp"
      },
      "outputs": [],
      "source": [
        "class HWRTransformerDecoder(nn.Module):\n",
        "    def __init__(self, total_nr_of_tokens):\n",
        "        super(HWRTransformerDecoder, self).__init__()\n",
        "\n",
        "        # batch first = false -> input should be [target_len, batch_size, nr_tokens]\n",
        "        # transformer decoder layers (4 stacked transformer encoder layers (4 headed attention))\n",
        "        self.trans_decoder1 = nn.TransformerDecoderLayer(d_model = 256, nhead = 4, dim_feedforward = 1024, dropout = 0.2)\n",
        "        self.trans_decoder2 = nn.TransformerDecoderLayer(d_model = 256, nhead = 4, dim_feedforward = 1024, dropout = 0.2)\n",
        "        self.trans_decoder3 = nn.TransformerDecoderLayer(d_model = 256, nhead = 4, dim_feedforward = 1024, dropout = 0.2)\n",
        "        self.trans_decoder4 = nn.TransformerDecoderLayer(d_model = 256, nhead = 4, dim_feedforward = 1024, dropout = 0.2)\n",
        "\n",
        "        # reshape output to number of possible tokens\n",
        "        self.decoder_out_dense = nn.Linear(256, total_nr_of_tokens)\n",
        "\n",
        "    def forward(self, decoder_in, encoder_out, target_mask):\n",
        "        # input encoder output and predicted chars into decoder\n",
        "        decoder_out = self.trans_decoder1(decoder_in, encoder_out, target_mask)\n",
        "        decoder_out = self.trans_decoder2(decoder_out, encoder_out, target_mask)\n",
        "        decoder_out = self.trans_decoder3(decoder_out, encoder_out, target_mask)\n",
        "        decoder_out = self.trans_decoder4(decoder_out, encoder_out, target_mask)\n",
        "\n",
        "        # dense layer after decoder to predict one of all tokens (CE Loss)\n",
        "        decoder_out = self.decoder_out_dense(decoder_out)\n",
        "\n",
        "        return decoder_out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_HRHiwayBXKp"
      },
      "outputs": [],
      "source": [
        "class HWRTransformer(nn.Module):\n",
        "    def __init__(self, input_height, input_width, total_nr_of_tokens, longest_label_size):\n",
        "        super(HWRTransformer, self).__init__()\n",
        "        # CNN backbone to extract optical features of input image\n",
        "        self.cnn = CNN(input_height, input_width)\n",
        "\n",
        "        # pre-encoder positional information\n",
        "        self.pos_encoding = SinPosEncoding(dimensionality = 256)\n",
        "\n",
        "        # Transformer encoder\n",
        "        self.transformer_encoder = HWRTransformerEncoder(total_nr_of_tokens)\n",
        "        # dense layer and logsoftmax for intermediate output (to backprop with CTC Loss)\n",
        "        self.encoder_out_dense = nn.Linear(256, total_nr_of_tokens)\n",
        "        self.encoder_out_logsoftmax = nn.LogSoftmax(dim = 2)\n",
        "\n",
        "        # character embedding (dim rule of thumb -> 4th sqrt of nr_embeddings: for ~80 = 3)\n",
        "        #      NOTE: wrong, appearantly dims (encoder output, target embedding) need to be the same\n",
        "        # <PAD> embedding idx = 0\n",
        "        self.char_embedding = nn.Embedding(total_nr_of_tokens, 256, padding_idx = 0)\n",
        "\n",
        "        # Transformer decoder\n",
        "        self.decoder_target_mask = self.make_target_mask(longest_label_size)\n",
        "        self.transformer_decoder = HWRTransformerDecoder(total_nr_of_tokens)\n",
        "\n",
        "    # create a target mask for decoder input\n",
        "    #   masks the future target characters from being seen by the model before they should\n",
        "    def make_target_mask(self, size):\n",
        "        mask = torch.zeros((size, size), dtype = torch.float32).to(device)\n",
        "\n",
        "        for i in range(size):\n",
        "            for j in range(size):\n",
        "                if (j > i):\n",
        "                    mask[i][j] = float('-inf')\n",
        "        # print(mask)\n",
        "        return mask\n",
        "\n",
        "    def forward(self, input_image, decoder_in_embed_idxs):\n",
        "        # forward through backbone convolutional neural network\n",
        "        cnn_out = self.cnn(input_image)\n",
        "\n",
        "        # add pre-encoder positional information\n",
        "        cnn_out = self.pos_encoding(cnn_out)\n",
        "\n",
        "        # forward through transformer encoder\n",
        "        encoder_out = self.transformer_encoder(cnn_out)\n",
        "\n",
        "        # print(\"Encoder out shape:\", encoder_out.shape)\n",
        "\n",
        "        # dense layer for intermediate output (to backprop with CTC Loss).\n",
        "        # clone, otherwise inplace operation error (compute graph messes up)\n",
        "        cloned_encoder_out = torch.clone(encoder_out)\n",
        "        interm_encoder_out = self.encoder_out_logsoftmax(self.encoder_out_dense(cloned_encoder_out))\n",
        "\n",
        "        # add pre-decoder positional information\n",
        "        encoder_out = self.pos_encoding(encoder_out)\n",
        "\n",
        "        # embed character indices for input into decoder\n",
        "        shifted_target = self.char_embedding(decoder_in_embed_idxs)\n",
        "\n",
        "        # reshape from (batch, seq_len, embed_dim) -> (seq_len, batch, embed_dim)\n",
        "        shifted_target = torch.permute(shifted_target, (1, 0, 2))\n",
        "\n",
        "        # add positional encoding to shifted targets\n",
        "        shifted_target = self.pos_encoding(shifted_target)\n",
        "\n",
        "        # forward through transformer decoder\n",
        "        decoder_out = self.transformer_decoder(shifted_target, encoder_out, self.decoder_target_mask)\n",
        "\n",
        "        return interm_encoder_out, decoder_out"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "kMje1iwwBXKq"
      },
      "source": [
        "### Initialize transformer model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xg2rdYGgBXKq"
      },
      "outputs": [],
      "source": [
        "hwr_transformer = HWRTransformer(INPUT_HEIGHT, input_width, len(char_to_idx_mapping), longest_label)\n",
        "\n",
        "hwr_transformer.load_state_dict(torch.load(\"/content/drive/MyDrive/model_train22.pth\", map_location = torch.device(device)))\n",
        "\n",
        "hwr_transformer = hwr_transformer.to(device)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "lcyndFsBEivy"
      },
      "source": [
        "### Train set remade every x epochs with 2000 entries so it can be saved in GPU memory (faster training)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dRnA7U7NEivy"
      },
      "outputs": [],
      "source": [
        "# dataloaders\n",
        "train_set = HandWritingDataset(False, train_data.sample(500), DATA_PATH, input_width, INPUT_HEIGHT, char_to_idx_mapping, longest_label, mean, std)\n",
        "train_loader = DataLoader(train_set, batch_size = BATCH_SIZE, shuffle = True, num_workers = 2, pin_memory = True)\n",
        "\n",
        "\n",
        "# test loader\n",
        "test_set = HandWritingDataset(True, test_data, DATA_PATH, input_width, INPUT_HEIGHT, char_to_idx_mapping, longest_label, mean, std)\n",
        "test_loader = DataLoader(test_set, batch_size = BATCH_SIZE, shuffle = True, num_workers = 2,  pin_memory = True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "JKnNEvZBBXKq"
      },
      "source": [
        "### Test with one image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6fHU5UiVBXKr"
      },
      "outputs": [],
      "source": [
        "# test image as tensor, test label as one hot encoded target characters, labels shifted right as indices for embedding table\n",
        "image, label_as_idxs, label_length, test_label_one_hot, label_as_idxs_shifted_right = train_set[0]\n",
        "image2, _, _, _, _, = test_set[0]\n",
        "\n",
        "\n",
        "image.to(device)\n",
        "label_as_idxs.to(device)\n",
        "label_length.to(device)\n",
        "test_label_one_hot.to(device)\n",
        "label_as_idxs_shifted_right.to(device)\n",
        "\n",
        "print(\"image tensor shape:\\n->\", image.shape)\n",
        "print(\"label as idxs shape:\\n->\", label_as_idxs.shape)\n",
        "print(\"label as idxs reversed:\\n->\", [train_set.idx_to_char_mapping[char.item()] for char in label_as_idxs])\n",
        "print(\"label length:\\n->\", label_length)\n",
        "print(\"one hot label shape:\\n->\", test_label_one_hot.shape)\n",
        "print(\"label shifted right embedding index tensor shape:\\n->\", label_as_idxs_shifted_right.shape)\n",
        "\n",
        "# create \"batch\" with single image\n",
        "test_image_batch = image.unsqueeze(0)\n",
        "labels_shifted_right_idxs_batch = label_as_idxs_shifted_right.unsqueeze(0)\n",
        "\n",
        "# show image\n",
        "plt.imshow(image[0, :, :].cpu(), cmap = \"gray\")\n",
        "\n",
        "# test label = <BOS> *sentence in tokens* <EOS> <PAD> <PAD> ...\n",
        "out1, out2 = hwr_transformer(test_image_batch.to(device), labels_shifted_right_idxs_batch.to(device))\n",
        "print(\"\\nShape of output of encoder (CTC Loss):\\n->\", out1.shape)\n",
        "print(\"Shape of decoder output (CE Loss):\\n->\", out2.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LvWZFGbs6QdY"
      },
      "outputs": [],
      "source": [
        "plt.imshow(image2[0, :, :].cpu(), cmap = \"gray\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8dYqT7MB8Ar0"
      },
      "outputs": [],
      "source": [
        "del train_set, train_loader, test_set, test_loader"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "vo0kBH1NBXKr"
      },
      "source": [
        "### Define hybrid loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LMc-jGx1BXKr"
      },
      "outputs": [],
      "source": [
        "class HybridLoss(nn.Module):\n",
        "    def __init__(self, balance, nr_of_classes):\n",
        "        super(HybridLoss, self).__init__()\n",
        "        # allignment probabilities over encoded input sequence and target sequence\n",
        "        #   labels are in conform with char to idx mapping (nr of classes - 1 indices)\n",
        "        #   so I decided to make the blank index the next unused idx mapping\n",
        "        self.interm_CTCloss = nn.CTCLoss()\n",
        "        # difference between decodeced input sequence and target sequence\n",
        "        self.output_CELoss = nn.CrossEntropyLoss()\n",
        "\n",
        "        # balance between CTC Loss and CE Loss (R: [0, 1])\n",
        "        if balance < 0 or balance > 1:\n",
        "            raise ValueError(\"Balance should be a value between 0 (only output CELoss) and 1 (only intermediate CTCLoss)\")\n",
        "        self.balance = balance\n",
        "\n",
        "    def setBalance(self, balance):\n",
        "        self.balance = balance\n",
        "\n",
        "    def forward(self, encoder_outputs, encoder_targets, label_lengths, decoder_outputs, decoder_targets):\n",
        "        # create input lengths tensor based on the size of the encoder output\n",
        "        input_lengths = torch.tensor([encoder_outputs.size(0)]).repeat(encoder_outputs.size(1))\n",
        "\n",
        "        # cross entropy needs (batch_size, seq_len, nr_tokens), right now its (seq_len, batch_size, nr_tokens)\n",
        "        decoder_outputs = torch.permute(decoder_outputs, (1, 0, 2))\n",
        "        decoder_targets = torch.permute(decoder_targets, (1, 0, 2))\n",
        "\n",
        "        interm_loss = self.interm_CTCloss(encoder_outputs, encoder_targets, input_lengths, label_lengths)\n",
        "        output_loss = self.output_CELoss(decoder_outputs, decoder_targets)\n",
        "\n",
        "        return (self.balance * interm_loss + (1 - self.balance) * output_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jY4lqZheBXKr"
      },
      "outputs": [],
      "source": [
        "# in the paper they only used balance to test efficiousness of the hybrid loss,\n",
        "#   and train with a balance of 0.5\n",
        "nr_of_classes = len(char_to_idx_mapping)\n",
        "print(nr_of_classes)\n",
        "hybrid_loss_func = HybridLoss(0.5, nr_of_classes)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "JbxpgLA5BXKs"
      },
      "source": [
        "### Define optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UF5a32bYBXKs"
      },
      "outputs": [],
      "source": [
        "optimizer = optim.Adam(hwr_transformer.parameters(), lr = 0.00025)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Mn-Z60aP-4Zi"
      },
      "source": [
        "### Scheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dWeHVigf-4IT"
      },
      "outputs": [],
      "source": [
        "# optimizer warm up\n",
        "scheduler_warm_up = optim.lr_scheduler.LinearLR(optimizer, start_factor = 1.0, end_factor = 0.9, total_iters = 50)\n",
        "# scheduler_warm_up = optim.lr_scheduler.LinearLR(optimizer, start_factor = 0.1, end_factor = 1.0, total_iters = 50)\n",
        "scheduler_decay = optim.lr_scheduler.LinearLR(optimizer, start_factor = 1.0, end_factor = 0.1, total_iters = 500)\n",
        "# scheduler_decay = optim.lr_scheduler.LinearLR(optimizer, start_factor = 1.0, end_factor = 0.1, total_iters = 750)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Qq5C1ODtBXKs"
      },
      "source": [
        "### Define train loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GCfGJqIzBXKs"
      },
      "outputs": [],
      "source": [
        "def train(model, train_data, test_data, loss_func, optim, scheduler_warm_up, scheduler_decay, epochs):\n",
        "# def train(model, train_data : pd.DataFrame, test_data : pd.DataFrame, loss_func, optim, scheduler_warm_up, scheduler_decay, epochs):\n",
        "    lowest_train_loss = np.inf\n",
        "    lowest_test_loss = np.inf\n",
        "    train_losses = []\n",
        "    test_losses = []\n",
        "\n",
        "    # # test dataset / loader\n",
        "    test_set = HandWritingDataset(True, test_data, DATA_PATH, input_width, INPUT_HEIGHT, char_to_idx_mapping, longest_label, mean, std)\n",
        "    test_loader = DataLoader(test_set, batch_size = BATCH_SIZE, shuffle = True)\n",
        "    train_set = []\n",
        "    train_loader = []\n",
        "\n",
        "    # init tensorboard writer\n",
        "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "    writer = SummaryWriter(f\"/content/drive/MyDrive/runs/IAM_transformer_{timestamp}\")\n",
        "    # writer = SummaryWriter(f\"/home/hkolstee/uniprojects/HWR/IAM_pipelines/runs/IAM_transformer_{timestamp}\")\n",
        "\n",
        "    for epoch in (progress_bar := tqdm(range(epochs))):\n",
        "        # create new train set every 10 epochs\n",
        "        if (epoch % 3 == 0):\n",
        "            # datasets\n",
        "            del train_set\n",
        "            train_set = HandWritingDataset(False, train_data.sample(n = 2000, ignore_index = True).drop(\"index\", axis = 1), DATA_PATH, input_width, INPUT_HEIGHT, char_to_idx_mapping, longest_label, mean, std)\n",
        "\n",
        "            # dataloaders\n",
        "            del train_loader\n",
        "            train_loader = DataLoader(train_set, batch_size = BATCH_SIZE, shuffle = True)\n",
        "\n",
        "        # add epoch info to progress bar\n",
        "        progress_bar.set_description(f\"Epoch {epoch}\")\n",
        "\n",
        "        # reset running losses\n",
        "        running_loss_train = 0\n",
        "        running_loss_test = 0\n",
        "\n",
        "        # train\n",
        "        model.train(True)\n",
        "        with torch.autograd.set_detect_anomaly(True):\n",
        "            for i, data in enumerate(train_loader):\n",
        "                if i != 0:\n",
        "                    del images\n",
        "                    del labels_as_idxs\n",
        "                    del labels_lengths\n",
        "                    del one_hot_labels\n",
        "                    del labels_as_idxs_shifted_right\n",
        "                # batched inputs (encoder = images, decoder = labels_shifted_rigth) and labels\n",
        "                images, labels_as_idxs, labels_lengths, one_hot_labels, labels_as_idxs_shifted_right = data\n",
        "                # move data to device\n",
        "                images = images.to(device)\n",
        "                labels_as_idxs = labels_as_idxs.to(device)\n",
        "                labels_lengths = labels_lengths.to(device)\n",
        "                one_hot_labels = one_hot_labels.to(device)\n",
        "                labels_as_idxs_shifted_right = labels_as_idxs_shifted_right.to(device)\n",
        "\n",
        "                # change one hot labels from (batch, seq_length, nr_tokens) to (seq_length, batch, nr_tokens)\n",
        "                one_hot_labels = torch.permute(one_hot_labels, (1, 0, 2))\n",
        "\n",
        "                # zero gradients before any calculations\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # predict\n",
        "                interm_outputs, decoder_outputs = model(images, labels_as_idxs_shifted_right)\n",
        "\n",
        "                # calc loss\n",
        "                loss = loss_func(interm_outputs.to(device), labels_as_idxs, labels_lengths, decoder_outputs, one_hot_labels)\n",
        "                # print(loss.item())\n",
        "\n",
        "                # backward pass for gradients\n",
        "                loss.backward()\n",
        "\n",
        "                # take step along loss gradients\n",
        "                optimizer.step()\n",
        "\n",
        "                # add to running loss\n",
        "                running_loss_train += loss.item()\n",
        "\n",
        "            # add to loss value lists\n",
        "            train_losses.append(running_loss_train / len(train_loader))\n",
        "            # check if lowest loss\n",
        "            if (train_losses[-1] < lowest_train_loss):\n",
        "                lowest_train_loss = train_losses[-1]\n",
        "                # Save model\n",
        "                torch.save(model.state_dict(), \"drive/MyDrive/model_train\" + \".pth\")\n",
        "\n",
        "        # test\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for j, data in enumerate(test_loader):\n",
        "                # batched inputs (encoder = images, decoder = labels_shifted_rigth) and labels\n",
        "                if j != 0:\n",
        "                    del images\n",
        "                    del labels_as_idxs\n",
        "                    del labels_lengths\n",
        "                    del one_hot_labels\n",
        "                    del labels_as_idxs_shifted_right\n",
        "                images, labels_as_idxs, labels_lengths, one_hot_labels, labels_as_idxs_shifted_right = data\n",
        "                labels_as_idxs = labels_as_idxs.to(device)\n",
        "                labels_lengths = labels_lengths.to(device)\n",
        "                one_hot_labels = one_hot_labels.to(device)\n",
        "                labels_as_idxs_shifted_right = labels_as_idxs_shifted_right.to(device)\n",
        "\n",
        "                # move data to device\n",
        "                images = images.to(device)\n",
        "\n",
        "                # change one hot labels from (batch, seq_length, nr_tokens) to (seq_length, batch, nr_tokens)\n",
        "                one_hot_labels = torch.permute(one_hot_labels, (1, 0, 2))\n",
        "\n",
        "                # predict\n",
        "                interm_outputs, decoder_outputs = model(images, labels_as_idxs_shifted_right)\n",
        "\n",
        "                # calc loss\n",
        "                loss = loss_func(interm_outputs.to(device), labels_as_idxs, labels_lengths, decoder_outputs, one_hot_labels)\n",
        "\n",
        "                # add to running loss\n",
        "                running_loss_test += loss.item()\n",
        "\n",
        "        # add to loss value lists\n",
        "        train_losses.append(running_loss_train / len(train_loader))\n",
        "        test_losses.append(running_loss_test / len(test_loader))\n",
        "\n",
        "        # learning rate schedulers\n",
        "        if (epoch < 50):\n",
        "            scheduler_warm_up.step()\n",
        "        else:\n",
        "            scheduler_decay.step()\n",
        "\n",
        "\n",
        "\n",
        "        # if lowest till now, save model (checkpointing)\n",
        "        if (test_losses[-1] < lowest_test_loss):\n",
        "            lowest_test_loss = test_losses[-1]\n",
        "            torch.save(model.state_dict(), \"drive/MyDrive/model_test\" + \".pth\")\n",
        "\n",
        "\n",
        "\n",
        "        # before next epoch: add last epoch info to progress bar\n",
        "        progress_bar.set_postfix({\"train_loss\": train_losses[-1], \"test_loss\": test_losses[-1], \"lr\": scheduler_warm_up.get_last_lr() if epoch < 50 else scheduler_decay.get_last_lr()})\n",
        "\n",
        "        # save to tensorboard writer\n",
        "        writer.add_scalars('Training vs. Validation Loss', { 'Training' : train_losses[-1], 'Validation' : test_losses[-1] }, epoch + 1)\n",
        "\n",
        "    return train_losses, test_losses"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "OL9u0xDFBXKt"
      },
      "source": [
        "### Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xChBB4d1J0c9"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir /content/drive/MyDrive/runs/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ZHLo-7lBXKt",
        "outputId": "810d4e81-52e0-44b4-e051-8f57712c169b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 0:   0%|          | 0/1500 [01:29<?, ?it/s]"
          ]
        }
      ],
      "source": [
        "train(hwr_transformer, train_data, test_data, hybrid_loss_func, optimizer, scheduler_warm_up, scheduler_decay, EPOCHS)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
